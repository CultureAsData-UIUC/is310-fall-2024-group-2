{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import csv\n",
    "banned_books_df = pd.read_csv('merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1362MB [00:06, 203.40MB/s]                                       \n"
     ]
    }
   ],
   "source": [
    "def decompress_gzip_to_file(compressed_file: str, uncompressed_file: str, chunk_size: int = 4 * 1024 * 1024) -> None:\n",
    "\t\"\"\"\n",
    "\tDecompress a gzipped file to a specified destination.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tcompressed_file (str): Path to the gzipped file.\n",
    "\t\tuncompressed_file (str): Path where the uncompressed data should be written.\n",
    "\t\tchunk_size (int, optional): Size of the chunks to be read from the compressed file. Defaults to 4MB.\n",
    "\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t\"\"\"\n",
    "\tif not os.path.exists(uncompressed_file):\n",
    "\t\twith gzip.open(compressed_file, 'rb') as f_in, open(uncompressed_file, 'wb') as f_out, tqdm(\n",
    "\t\t\tunit=\"MB\", total=os.path.getsize(compressed_file) / (chunk_size)) as pbar:\n",
    "\t\t\twhile True:\n",
    "\t\t\t\tchunk = f_in.read(chunk_size)\n",
    "\t\t\t\tif not chunk:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tf_out.write(chunk)\n",
    "\t\t\t\tpbar.update(1)\n",
    "\n",
    "# Example usage\n",
    "compressed_file = 'hathi_full_20241201.txt.gz'\n",
    "uncompressed_file = 'hathi_full_20241201.txt'\n",
    "decompress_gzip_to_file(compressed_file, uncompressed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/c_5vvt992871rr8qbqy645340000gq/T/ipykernel_73127/31703022.py:8: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  hathi_df = pd.read_csv(uncompressed_file, delimiter='\\t', names=headers, quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
      "/var/folders/fc/c_5vvt992871rr8qbqy645340000gq/T/ipykernel_73127/31703022.py:8: DtypeWarning: Columns (4,7,8,9,10,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hathi_df = pd.read_csv(uncompressed_file, delimiter='\\t', names=headers, quoting=csv.QUOTE_NONE, error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# Load the column headers from the hathi_field_list.txt file\n",
    "headers_file = 'hathi_field_list.txt'\n",
    "with open(headers_file, 'r') as f:\n",
    "    headers = f.read().strip().split('\\t')\n",
    "\n",
    "# Load the uncompressed file into a DataFrame using the headers\n",
    "try:\n",
    "    hathi_df = pd.read_csv(uncompressed_file, delimiter='\\t', names=headers, quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'htid': {0: 'mdp.39015018415946'},\n",
       " 'access': {0: 'deny'},\n",
       " 'rights': {0: 'ic'},\n",
       " 'ht_bib_key': {0: 1},\n",
       " 'description': {0: 'v.5'},\n",
       " 'source': {0: 'MIU'},\n",
       " 'source_bib_num': {0: '990000000010106381'},\n",
       " 'oclc_num': {0: '2779601'},\n",
       " 'isbn': {0: '8081281584,9788081281587'},\n",
       " 'issn': {0: nan},\n",
       " 'lccn': {0: '70518371'},\n",
       " 'title': {0: 'Slovenské vyst̕ahovalectvo / Zost. František Bielik a Elo Rákoš.'},\n",
       " 'imprint': {0: 'Matica Slovenská, SAV, t. Svornost̕, 1969-'},\n",
       " 'rights_reason_code': {0: 'bib'},\n",
       " 'rights_timestamp': {0: '2011-09-15 04:30:52'},\n",
       " 'us_gov_doc_flag': {0: 0},\n",
       " 'rights_date_used': {0: 9999},\n",
       " 'pub_place': {0: 'xo '},\n",
       " 'lang': {0: 'slo'},\n",
       " 'bib_fmt': {0: 'BK'},\n",
       " 'collection_code': {0: 'MIU'},\n",
       " 'content_provider_code': {0: 'umich'},\n",
       " 'responsible_entity_code': {0: 'umich'},\n",
       " 'digitization_agent_code': {0: 'google'},\n",
       " 'access_profile_code': {0: 'google'},\n",
       " 'author': {0: 'Bielik, František,'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hathi_df[0:1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Multiplication Is for White People\": Raising ...</td>\n",
       "      <td>Delpit, Lisa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#BlackLivesMatter: Protesting Racism</td>\n",
       "      <td>Thomas, Rachael L.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Hockey</td>\n",
       "      <td>Ukazu, Ngozi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Hockey (Check, Please! Series)</td>\n",
       "      <td>Ukazu, Ngozi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#MurderTrending (MurderTrending Series)</td>\n",
       "      <td>McNeil, Gretchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5889</th>\n",
       "      <td>yolo (Internet Girls Series)</td>\n",
       "      <td>Myracle, Lauren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5890</th>\n",
       "      <td>¡Solo pregunta!: Sé Diferente, Sé Valiente, Sé Tú</td>\n",
       "      <td>Sotomayor, Sonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>¡Vámonos! Let's Go!</td>\n",
       "      <td>Lainez, Rene Colato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>¿De Dónde Eres?/Where Are You From?</td>\n",
       "      <td>Mendez, Yamile Saied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2915 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title                Author\n",
       "0     \"Multiplication Is for White People\": Raising ...          Delpit, Lisa\n",
       "1                  #BlackLivesMatter: Protesting Racism    Thomas, Rachael L.\n",
       "3                                               #Hockey          Ukazu, Ngozi\n",
       "5                       #Hockey (Check, Please! Series)          Ukazu, Ngozi\n",
       "8               #MurderTrending (MurderTrending Series)      McNeil, Gretchen\n",
       "...                                                 ...                   ...\n",
       "5889                       yolo (Internet Girls Series)       Myracle, Lauren\n",
       "5890  ¡Solo pregunta!: Sé Diferente, Sé Valiente, Sé Tú      Sotomayor, Sonia\n",
       "5891                                ¡Vámonos! Let's Go!   Lainez, Rene Colato\n",
       "5892                ¿De Dónde Eres?/Where Are You From?  Mendez, Yamile Saied\n",
       "5893                                                NaN                   NaN\n",
       "\n",
       "[2915 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banned_books_df[['Title', 'Author']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-work-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
