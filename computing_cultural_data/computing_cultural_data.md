Computational methods list: 
Lhaye: Text mining
Rebeca: Geographical Information Systems
Avery: social network analysis

Cultural data list:
Ryan: Emotional data 
Henry: Spatial Data

# Ryan:
## Bibliographic information: 
Citation: 
Bosch, N., D’Mello, S. The Affective Experience of Novice Computer Programmers. Int J Artif Intell Educ 27, 181–206 (2017). https://doi.org/10.1007/s40593-015-0069-5
This article was published in the international journal of artificial intelligence in education in 2017. It was written by University of Illinois Urbana-Champaign Information Sciences Professor Nigel Bosch and University of Colorado Boulder Computer Science Professor Sidney D’Mello. 


## Description of cultural data:
	In this piece, the authors are using emotional data to determine how their research participants are feeling when doing coding tasks on a computer. They assign a common emotion to approximately every 15 seconds of time or when a student looks back on the video and marks a change in their emotional state. The researchers pile this data together into lists and mark standard deviations and means of emotions during different tasks. They also look into the transitions from one emotion to the next.

## Summary of argument being made:
In this paper, the authors look into how novice computer programmers feel as they learn to code, and the resulting model of affective states during the learning process. They use the emotional data to try to understand exactly how best to design a future computer science learning environment for those who haven’t learned any computer science before. For example, participants didn’t learn as much when they felt frustrated or confused. The emotional transitions and affective states can be used to have changing environments where computers detect facial features of a participant, and make it easier if they’re stuck.

## Code available: 
There isn’t any direct code for this paper publicly available from what I can tell, but there is data on the different affective states, their correlations, and transitions. 
The first table shows what primary state of emotion is felt most commonly on average during each of the stages; a scaffolding phase (which I see most like a learning phase), Fadeout phase (similar to a final test with no hints available), and overall emotions. 
The second table shows the primary affective states and which secondary affective state a participant was feeling on average alongside each primary one. 
The third table shows the lift or probability above random chance that each combination of two emotional states occurred alongside one another as compared to the results in the experiment. 
The fourth table uses t-tests to determine if one affective state tends to imply another affective state. 
Table five goes further in depth about the transition from one affective state to another.
The last table (seven) shows how affective states related to a student’s learning outcome. 

## Assessment of most interesting parts:
I found this scholarship very interesting in terms of trying to understand emotionally what new computer programmers are feeling, instead of just what they’re thinking or trying to decide whether they’re successful at computer science based on other solid factors such as test scores. I would recognize this as computerized psychology. I would classify this as such because it goes into emotion and learning styles, but is trying its hardest to be accurate and computationally sound. 

## Assessment of how this is useful for our group project:
I think this scholarship might be useful for our group project because our group project is working with a lot of data that are related to one another in different ways, and we may need to step back and look at the bigger picture of why books are being banned, and if there are certain affective factors or models that could explain the banning of these books. 

—--------------------------------------------------------------------------
# Henry:

## Bibliography
Citation: Blundell, David, Lin, Ching-chih, Morris, James. “Spatial Humanities: An Integrated Approach to Spatiotemporal Research.” Computational Social Sciences, 1 Jan. 2018, pp. 263–288, https://doi.org/10.1007/978-3-319-95465-3_14. Accessed 26 Dec. 2023.

## Cultural Data 

This spatial humanities project is vast and utilizes data from a wide array of sources to create visualizations that can clarify history. The primary focus is humanities and culture in Asia. To create this data, a team of researchers, coders, GIS technicians, historians, and anthropologists collaborate to foster an accurate understanding of Asian culture. Another component of this project is accessibility, average people can download the technologies and contribute to the maps that are being created. Through this avenue, culture isn’t defined by one team, rather, it is akin to a survey of a region from a number of perspectives. The interconnectedness encourages expression, which allows for data that had once never existed to be documented. 

## Key Argument
The primary argument of this book is that Integrated humanities data and research is effective in determining patterns and finding the lost pieces of history and culture. They argue that the relatively matured “digital humanities” can be rejuvenated by modern GIS technologies. When combined, significant cultural items and sites can be digitized and available for all to observe and modify. This process encourages evolution and updates when new data is discovered. After all, we update our textbooks based on new discoveries and social change. By having an integrated system, we can stay up to date on new historical and cultural developments.

## Code Availability

Although I couldn’t find any github repositories (a lot of the links are in a different language) there are plenty of coding methods behind this project. As we have established, innovative practices are in place to collect data. A prominent coding practice they use is text digitization. For example, they can effectively digitize gravestones by use of a drone and normal text analysis scripting. A lot of the data comes from geo-photos that are then archived into the Data Archiving and Network Services (DANS). Afterwards, the coding team uses R to organize the data and analyze patterns of Asian history such as settlement, economics, and trade networks. R is a prominent statistical coding program, thus, I can imagine it works similarly to python’s data libraries. The DANS archive is available for public use and includes hundreds of csv files. They also digitize old maps and stone tablets which is similar to the newspaper restoration project we discussed in class. By using this data, GIS technicians can also create entirely new maps to visualize the data so it can reach a larger audience. These factors add to a massive cultural database, one center full of an endless stream of information. Ultimately, a lot of the code is open source and possible because of the emphasis on collaboration. 

## Interesting Aspects
I would categorize this as a mix of computational science and social sciences. Behind the data rests culture dating back hundreds of years ago. I love how the project is open source and features a flexible database that allows us to analyze ancient religious and cultural phenomena. In addition, I believe that the incorporation of GIS mapping technology entices others to contribute because the results can be displayed. For example, developers have used this data to create apps for users to find religious temples and monuments. There is something special about being a part of the process and seeing its beautiful and comprehensive end result. 

## Application to Project
I think the clearest application to our project is the aspect of visualization and collaboration. To me, this project is evidence for what a team of motivated individuals can accomplish. We are all human and have unique perspectives, especially when it comes to the naturally political/social nature of banned books. Moving forward, no idea and potential auditions are off the table. By encouraging individuality and collaboration, we can build a project that encapsulates a multitude of backgrounds and opinions, leading to a true digital humanities project. One that can be used to identify and track patterns in this culture dilemma that our libraries are facing.


—---------------------------------------------------------------------------------------------

# Avery:
## Bibliographic Information:
Ye, Dan. Svoboda, Pennisi. “Analyzing interactions in online discussions through social network analysis”. Journal of Computer Assisted Learning, January 25, 2022. https://doi.org/10.1111/jcal.12648. 
Computational Method:

Social Network Analysis is a computational method with a variety of applications possible to the users of this method. Primarily, it is used as a tool to study relationships that people or groups have with one another. One popular method of this would be to gather similar information from people of different cultural backgrounds, and then compare. In this piece, the design is an actual research study in which students are willingly participating. The study is meant to identify students’ online discussion, and use that information to compare to the students’ performance in their classes. The study was carried out by collecting data from online discussion forums for a specific course. They then used the data to evaluate how these discussions affect a student’s grade in said course. 

## Main Argument:
Through Social Network Analysis, the author’s of this study argue that there is positive correlation between the social interactions in online discussions and a student’s learning performance. In simpler terms, the more a student spends interacting with peers in these discussions, the better their grade will be. This study used Social Network Analysis (as well as regression) to come to this conclusion. The authors’ used the information in these forums to assess participants who were actively present in these discussions. They used “centrality measurements” to evaluate the importance of each participant (who was contributing the most relevant information). They also used algorithms to estimate which specific conversations were the most impactful on students’ grades. Lastly, they labeled the participants as in-degree or out-degree. In-degree means that the student received more comments than they commented on other students’ posts. Out-degree means that the student commented more on other students’ posts than they received on theirs. The authors’ gauged that out-degree students had more to contribute to these discussions, and subsequently did better in the course.

## Code Available:
The authors’ did not make their data or code publicly available from what I can tell. Regardless, they detailed every method they took to analyze this data, and then displayed the results. They used a popular social network analysis tool called UCINET to evaluate the data collected. They were able to calculate things like network density, transitivity, clustering coefficients, cliques and centrality measurements. They also performed some linear regression models in SPSS to identify relationships between centrality measures and final grades. Some of the results they display are tables detailing things like the type of person (in-degree/out-degree) and their final grades. They also have tables ranking the different discussions and how the degree to which they affected the participants' grades. They also analyzed discussions to identify cliques (small groups that commonly interacted with each other). These cliques were then compared to see which ones had the best grades in the course. The authors’ detail the language and results of their analysis, but they didn’t make their data available to use.

## Interesting Aspects:
I would categorize the techniques used in this article as a computational social science. They authors’ are able to analyze different aspects of people to assess how that affects them. In this article, people are assessed based on their willingness to participate in a discussion, as well as their ability to find peers to discuss with. Essentially, this is a study of sociality, and the effects of being social/anti-social. These methods could be used for many other applications as well, like comparing discussions across different cultures and languages. What I found most interesting about this article was the ability to create cliques from the data. The authors’ were able to construct a web of different groups for this study. They even went as far to select important members/leaders of these cliques and estimate how they affected their groups’ grades. I also found their results interesting, given that almost all of their students would participate in small discussions with the same people. I didn’t expect everyone to latch onto a group in only 4 weeks, and then converse with only each other.

## Applications for Project:
I believe that the clustering and creation of cliques present in this article is something that we can use for our final project. Specifically, in regards to book banning, I think it would be interesting to find cliques between the states. We can look at the types of books that a state banned, and compare. For example, if Florida banned a lot of youth LGBTQ+ books, we could compare this trend with other states, and then form groups based on which books they ban. Maybe some states target books on critical race theory. Maybe some states target books on violence and abuse. These methods could help to create a visualization in which we can group together the different states into different categories. The regression techniques in this article would also be very useful to use. It provides new ways to find predictors for a regression model, like how we can create density variables. Overall, I believe this article will be a useful tool in uncovering the trends with book banning by each state.
 —-------------------------------------------------------------------------------
# Lhaye:
## Bibliographic Information:
Irfan R, King CK, Grages D, et al. A survey on text mining in social networks. The Knowledge Engineering Review. 2015;30(2):157-170. doi:10.1017/S0269888914000277

## Computational Method:
Text mining is a computational method that extracts information from platforms that involve text, to later analyze and put into more structured datasets. This type of method takes in different steps to later find trends and within the texts found. It involves other types of methods such as preprocessing, which in general just cleans the text found to be easier to read through coding. From there it’ll be easier to run different types of data analysis on what we find through clusters or classifications. This overall gives us the ability to interpret what we find from our analysis and find trends and information from what we gather.

## Main Argument:
In the article, the main argument suggests that text mining is beneficial when it comes to analyzing social networks as it allows insight into human interactions, group identification and even human thinking. However, challenges such as data volume, real-time processing, privacy ethics, and data quality can hinder efficiency. For example, with there being so much data within these different social media platforms, there is an overwhelming amount to analyze. Social media is constant, there are comments and posts being made every second, so analyzing text real-time can be a problem when you want to analyze that trend. With all that, trying to figure out the ethics and privacy on what you can and can't retrieve, hinders the overall analysis even more. The article tries to find solutions to these problems through possible processing algorithms that can handle large datasets and finding other tools that allow a more enhancive real-time processing.

## Code Available:
The article does not have any code available. However, the article gives a detailed explanation of clustering processing for the data, analyzing how to use hierarchical clustering ,partitional clustering and k-means for text mining. For example, in the text it informs about hierarchical clustering and how it is used to organize data into tree-like structures, which showcase how different data points are related. Within hierarchical clustering, they use two different approaches such as agglomerative which just takes each individual data and merges them together, compared to the divisive method which uses a top-down approach that takes all the data points into one cluster and splits them into smaller groups.These explanations allow an insight into how the code might be organized and what algorithms might be implemented.

## Applications for Project:
I think text-mining would go well with our project as it can help us see what type of language or words are often found within the banned books. Text mining though can be a bit time consuming when it comes to coding, because there is a lot of coding that the team might not be as familiar with such as clustering and other tools. If we decided to implement text mining, we would try to look into sights that might explain more on the coding basics of it. With the use of text mining, we would use the dataset we have now with the book bans from 2021-2024 and have to find a way to split each book out and analyze the words used through algorithms and the use of clustering provided in the article

----------------------------------------------------------------------------------------

# Rebeca:

## Bibliographic Information: 
Graham, S. R., Carlton, C., Gaede, D., & Jamison, B. (2011). THE BENEFITS OF USING GEOGRAPHIC INFORMATION SYSTEMS AS A COMMUNITY ASSESSMENT TOOL. Public Health Reports (1974-), 126(2), 298–303. http://www.jstor.org/stable/41639361 

## Computational Method: 
The computational method that was chosen is Geographical Information Systems (GIS). This process is used to find and bring light to community assets while displaying patterns within the data that were not previously seen before. More generally, this is applied to any geographical information. Through this tool, we are able to see what relationships and hotspots are within a certain geographical area. Another benefit of this tool is that it is more simple to use than other methods of data presentation. Due to this simplicity, people are able to make evidence-based decisions and communities are able to adapt to solve any issues present. 

## Main Argument: 
This journal focuses on using a geographical information system within a community to map out its assets, relationships, and needs. The authors were testing the benefits of using a GIS in the  Westside community of San Bernardino, California, and whether that will solve any issues, in this case public health, from the information provided by the GIS. This community is in close proximity to several universities. The authors collaborated with the CBO of the community when gathering data and presented their findings to them. The results found that there was a large number of junk-food establishments, including liquor stores, and a lack of healthy food within the area. Another point that was noted was the high number of other CBOs and religious-based organizations, who could be potential partners in the future. To add, there was a high number of advertisements that presented negative information, like bail bonds, R-rated movies, and gambling. Lastly, the GIS found that there were many environmental hazards in the community, like areas with broken glass and needles. These findings lead to the CBO to create positive change and solve these problems in the community. 

## Code Availability: 
From what is seen in the journal, the code was not publicly available by the authors. Even with that being the case, the authors noted their process in collecting the information, how they analyzed the data, and what the results were in their findings. The data collection process consisted of windshield surveys, key information interviews, and ethnography to find the areas with the most concern. They also used Trimble Recon GPS units and students created data dictionaries through entering data in different categories. In the data analysis, the authors used Environmental systems Research Institutes ArcView 9.2 GIS software to identify findings from maps and aggregated data. The authors then presented their findings to the CBO leadership. 

## Interesting Findings:
There are some interesting points made throughout the journal. One of them being how useful GIS are to both large and small communities. In either case, being able to see a visualization of what is found can be extremely useful and powerful because it can bring to light assets and relationships that were not seen before. When those issues are brought up with the help of the GIS, then leaders and communities can begin to work together on solving that issue. It was also interesting to see how the CBO leaders suspected that there was a problem with the community’s access to healthy options of food and never really being able to prove that right or not, until the GIS. This shows the importance of these systems as they are able to create foundational evidence, relationships, and patterns that might have not been seen before. 

## Applications for the Project:
Using Geographical Information Systems would be a great addition to the project, as our group has a large focus on geographic data in relation to book banning. One visualization that we were thinking of including when completing this project is to create a map of the United States and include what books were banned, what the reason for banning said books was, their genres, and other information over the years. Not only would this create a stunning visual representation, it would also be very impactful as it would highlight underlying issues in this topic that are not seen as much. Of course, this is not the only use that we would have. There are multiple possibilities with the use of the GIS. 
